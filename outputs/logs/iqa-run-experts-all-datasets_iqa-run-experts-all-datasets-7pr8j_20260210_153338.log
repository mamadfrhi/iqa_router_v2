Running dataset=livec data_root=/local/data/b-s-farrahi phase=test
Loading dataset livec from /local/data/b-s-farrahi ...
/opt/conda/lib/python3.10/site-packages/torch/hub.py:884: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)
  return torch.load(cached_file, map_location=map_location, weights_only=weights_only)
arniqa: 10/10 processed
Saved 10 rows to /local/outputs/preds/livec/test/preds_arniqa.csv
/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/q-future/Compare2Score:
- configuration_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/q-future/Compare2Score:
- modeling_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 793.66it/s]
Instantiating LlamaAttention without passing `layer_idx` is not recommended and will to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` when creating this class.
/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.19s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.96s/it]
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
compare2score: 10/10 processed
Saved 10 rows to /local/outputs/preds/livec/test/preds_compare2score.csv
Loading pretrained model QualiCLIP from /pip-cache/torch/hub/pyiqa/QualiCLIP%2B_koniq.pth
qualiclip+: 10/10 processed
Saved 10 rows to /local/outputs/preds/livec/test/preds_qualiclip+.csv
/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/q-future/one-align:
- configuration_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/q-future/one-align:
- modeling_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1406.07it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.32s/it]
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
qalign: 10/10 processed
Saved 10 rows to /local/outputs/preds/livec/test/preds_qalign.csv
clipiqa+: 10/10 processed
Saved 10 rows to /local/outputs/preds/livec/test/preds_clipiqa+.csv
/opt/conda/lib/python3.10/site-packages/timm/models/layers/__init__.py:49: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Loading pretrained model MANIQA from /pip-cache/torch/hub/pyiqa/ckpt_koniq10k.pt
maniqa: 10/10 processed
Saved 10 rows to /local/outputs/preds/livec/test/preds_maniqa.csv
Loading pretrained model MUSIQ from /pip-cache/torch/hub/pyiqa/musiq_koniq_ckpt-e95806b9.pth
musiq: 10/10 processed
Saved 10 rows to /local/outputs/preds/livec/test/preds_musiq.csv
Loading pretrained model CFANet from /pip-cache/torch/hub/pyiqa/cfanet_nr_koniq_res50-9a73138b.pth
topiq_nr: 10/10 processed
Saved 10 rows to /local/outputs/preds/livec/test/preds_topiq_nr.csv
Loading pretrained model TReS from /pip-cache/torch/hub/pyiqa/tres_koniq-f0502926.pth
tres: 10/10 processed
Saved 10 rows to /local/outputs/preds/livec/test/preds_tres.csv
Loading pretrained model LIQE from /pip-cache/torch/hub/pyiqa/liqe_koniq.pt
liqe: 10/10 processed
Saved 10 rows to /local/outputs/preds/livec/test/preds_liqe.csv
Loading pretrained model HyperNet from /pip-cache/torch/hub/pyiqa/HyperIQA-resnet50-koniq10k-c96c41b1.pth
hyperiqa: 10/10 processed
Saved 10 rows to /local/outputs/preds/livec/test/preds_hyperiqa.csv
Running dataset=koniq10k data_root=/local/data/b-s-farrahi phase=test
Loading dataset koniq10k from /local/data/b-s-farrahi ...
/opt/conda/lib/python3.10/site-packages/torch/hub.py:884: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)
  return torch.load(cached_file, map_location=map_location, weights_only=weights_only)
arniqa: 10/10 processed
Saved 10 rows to /local/outputs/preds/koniq10k/test/preds_arniqa.csv
/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/q-future/Compare2Score:
- configuration_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/q-future/Compare2Score:
- modeling_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2096.89it/s]
Instantiating LlamaAttention without passing `layer_idx` is not recommended and will to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` when creating this class.
/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.13s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.10s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.73s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.87s/it]
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
compare2score: 10/10 processed
Saved 10 rows to /local/outputs/preds/koniq10k/test/preds_compare2score.csv
Loading pretrained model QualiCLIP from /pip-cache/torch/hub/pyiqa/QualiCLIP%2B_koniq.pth
qualiclip+: 10/10 processed
Saved 10 rows to /local/outputs/preds/koniq10k/test/preds_qualiclip+.csv
/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/q-future/one-align:
- configuration_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/q-future/one-align:
- modeling_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1929.75it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.08s/it]
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
qalign: 10/10 processed
Saved 10 rows to /local/outputs/preds/koniq10k/test/preds_qalign.csv
clipiqa+: 10/10 processed
Saved 10 rows to /local/outputs/preds/koniq10k/test/preds_clipiqa+.csv
/opt/conda/lib/python3.10/site-packages/timm/models/layers/__init__.py:49: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Loading pretrained model MANIQA from /pip-cache/torch/hub/pyiqa/ckpt_koniq10k.pt
maniqa: 10/10 processed
Saved 10 rows to /local/outputs/preds/koniq10k/test/preds_maniqa.csv
Loading pretrained model MUSIQ from /pip-cache/torch/hub/pyiqa/musiq_koniq_ckpt-e95806b9.pth
musiq: 10/10 processed
Saved 10 rows to /local/outputs/preds/koniq10k/test/preds_musiq.csv
Loading pretrained model CFANet from /pip-cache/torch/hub/pyiqa/cfanet_nr_koniq_res50-9a73138b.pth
topiq_nr: 10/10 processed
Saved 10 rows to /local/outputs/preds/koniq10k/test/preds_topiq_nr.csv
Loading pretrained model TReS from /pip-cache/torch/hub/pyiqa/tres_koniq-f0502926.pth
tres: 10/10 processed
Saved 10 rows to /local/outputs/preds/koniq10k/test/preds_tres.csv
Loading pretrained model LIQE from /pip-cache/torch/hub/pyiqa/liqe_koniq.pt
liqe: 10/10 processed
Saved 10 rows to /local/outputs/preds/koniq10k/test/preds_liqe.csv
Loading pretrained model HyperNet from /pip-cache/torch/hub/pyiqa/HyperIQA-resnet50-koniq10k-c96c41b1.pth
hyperiqa: 10/10 processed
Saved 10 rows to /local/outputs/preds/koniq10k/test/preds_hyperiqa.csv
Running dataset=tid2013 data_root=/local/data/b-s-farrahi phase=test
Loading dataset tid2013 from /local/data/b-s-farrahi ...
/opt/conda/lib/python3.10/site-packages/torch/hub.py:884: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)
  return torch.load(cached_file, map_location=map_location, weights_only=weights_only)
arniqa: 10/10 processed
Saved 10 rows to /local/outputs/preds/tid2013/test/preds_arniqa.csv
/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/q-future/Compare2Score:
- configuration_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/q-future/Compare2Score:
- modeling_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1943.83it/s]
Instantiating LlamaAttention without passing `layer_idx` is not recommended and will to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` when creating this class.
/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.17s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.16s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.94s/it]
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
compare2score: 10/10 processed
Saved 10 rows to /local/outputs/preds/tid2013/test/preds_compare2score.csv
Loading pretrained model QualiCLIP from /pip-cache/torch/hub/pyiqa/QualiCLIP%2B_koniq.pth
qualiclip+: 10/10 processed
Saved 10 rows to /local/outputs/preds/tid2013/test/preds_qualiclip+.csv
/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/q-future/one-align:
- configuration_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/q-future/one-align:
- modeling_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1931.08it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.73s/it]
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
qalign: 10/10 processed
Saved 10 rows to /local/outputs/preds/tid2013/test/preds_qalign.csv
clipiqa+: 10/10 processed
Saved 10 rows to /local/outputs/preds/tid2013/test/preds_clipiqa+.csv
/opt/conda/lib/python3.10/site-packages/timm/models/layers/__init__.py:49: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Loading pretrained model MANIQA from /pip-cache/torch/hub/pyiqa/ckpt_koniq10k.pt
maniqa: 10/10 processed
Saved 10 rows to /local/outputs/preds/tid2013/test/preds_maniqa.csv
Loading pretrained model MUSIQ from /pip-cache/torch/hub/pyiqa/musiq_koniq_ckpt-e95806b9.pth
musiq: 10/10 processed
Saved 10 rows to /local/outputs/preds/tid2013/test/preds_musiq.csv
Loading pretrained model CFANet from /pip-cache/torch/hub/pyiqa/cfanet_nr_koniq_res50-9a73138b.pth
topiq_nr: 10/10 processed
Saved 10 rows to /local/outputs/preds/tid2013/test/preds_topiq_nr.csv
Loading pretrained model TReS from /pip-cache/torch/hub/pyiqa/tres_koniq-f0502926.pth
tres: 10/10 processed
Saved 10 rows to /local/outputs/preds/tid2013/test/preds_tres.csv
Loading pretrained model LIQE from /pip-cache/torch/hub/pyiqa/liqe_koniq.pt
liqe: 10/10 processed
Saved 10 rows to /local/outputs/preds/tid2013/test/preds_liqe.csv
Loading pretrained model HyperNet from /pip-cache/torch/hub/pyiqa/HyperIQA-resnet50-koniq10k-c96c41b1.pth
hyperiqa: 10/10 processed
Saved 10 rows to /local/outputs/preds/tid2013/test/preds_hyperiqa.csv
Running dataset=spaq data_root=/local/data/b-s-farrahi phase=test
Loading dataset spaq from /local/data/b-s-farrahi ...
/opt/conda/lib/python3.10/site-packages/torch/hub.py:884: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)
  return torch.load(cached_file, map_location=map_location, weights_only=weights_only)
arniqa: 10/10 processed
Saved 10 rows to /local/outputs/preds/spaq/test/preds_arniqa.csv
/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/q-future/Compare2Score:
- configuration_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/q-future/Compare2Score:
- modeling_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1913.24it/s]
Instantiating LlamaAttention without passing `layer_idx` is not recommended and will to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` when creating this class.
/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.15s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.17s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.93s/it]
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
compare2score: 10/10 processed
Saved 10 rows to /local/outputs/preds/spaq/test/preds_compare2score.csv
Loading pretrained model QualiCLIP from /pip-cache/torch/hub/pyiqa/QualiCLIP%2B_koniq.pth
qualiclip+: 10/10 processed
Saved 10 rows to /local/outputs/preds/spaq/test/preds_qualiclip+.csv
/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/q-future/one-align:
- configuration_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/q-future/one-align:
- modeling_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 2007.32it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.02s/it]
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
qalign: 10/10 processed
Saved 10 rows to /local/outputs/preds/spaq/test/preds_qalign.csv
clipiqa+: 10/10 processed
Saved 10 rows to /local/outputs/preds/spaq/test/preds_clipiqa+.csv
/opt/conda/lib/python3.10/site-packages/timm/models/layers/__init__.py:49: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Loading pretrained model MANIQA from /pip-cache/torch/hub/pyiqa/ckpt_koniq10k.pt
maniqa: 10/10 processed
Saved 10 rows to /local/outputs/preds/spaq/test/preds_maniqa.csv
Loading pretrained model MUSIQ from /pip-cache/torch/hub/pyiqa/musiq_koniq_ckpt-e95806b9.pth
musiq: 10/10 processed
Saved 10 rows to /local/outputs/preds/spaq/test/preds_musiq.csv
Loading pretrained model CFANet from /pip-cache/torch/hub/pyiqa/cfanet_nr_koniq_res50-9a73138b.pth
topiq_nr: 10/10 processed
Saved 10 rows to /local/outputs/preds/spaq/test/preds_topiq_nr.csv
Loading pretrained model TReS from /pip-cache/torch/hub/pyiqa/tres_koniq-f0502926.pth
tres: 10/10 processed
Saved 10 rows to /local/outputs/preds/spaq/test/preds_tres.csv
Loading pretrained model LIQE from /pip-cache/torch/hub/pyiqa/liqe_koniq.pt
liqe: 10/10 processed
Saved 10 rows to /local/outputs/preds/spaq/test/preds_liqe.csv
Loading pretrained model HyperNet from /pip-cache/torch/hub/pyiqa/HyperIQA-resnet50-koniq10k-c96c41b1.pth
hyperiqa: 10/10 processed
Saved 10 rows to /local/outputs/preds/spaq/test/preds_hyperiqa.csv
Running dataset=kadid10k data_root=/local/data/b-s-farrahi phase=test
Loading dataset kadid10k from /local/data/b-s-farrahi ...
/opt/conda/lib/python3.10/site-packages/torch/hub.py:884: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)
  return torch.load(cached_file, map_location=map_location, weights_only=weights_only)
arniqa: 10/10 processed
Saved 10 rows to /local/outputs/preds/kadid10k/test/preds_arniqa.csv
/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/q-future/Compare2Score:
- configuration_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/q-future/Compare2Score:
- modeling_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2087.24it/s]
Instantiating LlamaAttention without passing `layer_idx` is not recommended and will to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` when creating this class.
/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.32s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.04s/it]
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
compare2score: 10/10 processed
Saved 10 rows to /local/outputs/preds/kadid10k/test/preds_compare2score.csv
Loading pretrained model QualiCLIP from /pip-cache/torch/hub/pyiqa/QualiCLIP%2B_koniq.pth
qualiclip+: 10/10 processed
Saved 10 rows to /local/outputs/preds/kadid10k/test/preds_qualiclip+.csv
/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/q-future/one-align:
- configuration_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/q-future/one-align:
- modeling_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 270.18it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.51s/it]
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
qalign: 10/10 processed
Saved 10 rows to /local/outputs/preds/kadid10k/test/preds_qalign.csv
clipiqa+: 10/10 processed
Saved 10 rows to /local/outputs/preds/kadid10k/test/preds_clipiqa+.csv
/opt/conda/lib/python3.10/site-packages/timm/models/layers/__init__.py:49: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Loading pretrained model MANIQA from /pip-cache/torch/hub/pyiqa/ckpt_koniq10k.pt
maniqa: 10/10 processed
Saved 10 rows to /local/outputs/preds/kadid10k/test/preds_maniqa.csv
Loading pretrained model MUSIQ from /pip-cache/torch/hub/pyiqa/musiq_koniq_ckpt-e95806b9.pth
musiq: 10/10 processed
Saved 10 rows to /local/outputs/preds/kadid10k/test/preds_musiq.csv
Loading pretrained model CFANet from /pip-cache/torch/hub/pyiqa/cfanet_nr_koniq_res50-9a73138b.pth
topiq_nr: 10/10 processed
Saved 10 rows to /local/outputs/preds/kadid10k/test/preds_topiq_nr.csv
Loading pretrained model TReS from /pip-cache/torch/hub/pyiqa/tres_koniq-f0502926.pth
tres: 10/10 processed
Saved 10 rows to /local/outputs/preds/kadid10k/test/preds_tres.csv
Loading pretrained model LIQE from /pip-cache/torch/hub/pyiqa/liqe_koniq.pt
liqe: 10/10 processed
Saved 10 rows to /local/outputs/preds/kadid10k/test/preds_liqe.csv
Loading pretrained model HyperNet from /pip-cache/torch/hub/pyiqa/HyperIQA-resnet50-koniq10k-c96c41b1.pth
hyperiqa: 10/10 processed
Saved 10 rows to /local/outputs/preds/kadid10k/test/preds_hyperiqa.csv
Running dataset=tid2013 data_root=/local/data/b-s-farrahi phase=test
Loading dataset tid2013 from /local/data/b-s-farrahi ...
/opt/conda/lib/python3.10/site-packages/torch/hub.py:884: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)
  return torch.load(cached_file, map_location=map_location, weights_only=weights_only)
Saved 0 rows to /local/outputs/preds/tid2013/test/preds_arniqa.csv
/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/q-future/Compare2Score:
- configuration_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/q-future/Compare2Score:
- modeling_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1911.72it/s]
Instantiating LlamaAttention without passing `layer_idx` is not recommended and will to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` when creating this class.
/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.27s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.00s/it]
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Saved 0 rows to /local/outputs/preds/tid2013/test/preds_compare2score.csv
Loading pretrained model QualiCLIP from /pip-cache/torch/hub/pyiqa/QualiCLIP%2B_koniq.pth
Saved 0 rows to /local/outputs/preds/tid2013/test/preds_qualiclip+.csv
/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/q-future/one-align:
- configuration_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/q-future/one-align:
- modeling_mplug_owl2.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 2100.30it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.12s/it]
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Saved 0 rows to /local/outputs/preds/tid2013/test/preds_qalign.csv
Saved 0 rows to /local/outputs/preds/tid2013/test/preds_clipiqa+.csv
/opt/conda/lib/python3.10/site-packages/timm/models/layers/__init__.py:49: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Loading pretrained model MANIQA from /pip-cache/torch/hub/pyiqa/ckpt_koniq10k.pt
Saved 0 rows to /local/outputs/preds/tid2013/test/preds_maniqa.csv
Loading pretrained model MUSIQ from /pip-cache/torch/hub/pyiqa/musiq_koniq_ckpt-e95806b9.pth
Saved 0 rows to /local/outputs/preds/tid2013/test/preds_musiq.csv
Loading pretrained model CFANet from /pip-cache/torch/hub/pyiqa/cfanet_nr_koniq_res50-9a73138b.pth
Saved 0 rows to /local/outputs/preds/tid2013/test/preds_topiq_nr.csv
Loading pretrained model TReS from /pip-cache/torch/hub/pyiqa/tres_koniq-f0502926.pth
Saved 0 rows to /local/outputs/preds/tid2013/test/preds_tres.csv
Loading pretrained model LIQE from /pip-cache/torch/hub/pyiqa/liqe_koniq.pt
Saved 0 rows to /local/outputs/preds/tid2013/test/preds_liqe.csv
Loading pretrained model HyperNet from /pip-cache/torch/hub/pyiqa/HyperIQA-resnet50-koniq10k-c96c41b1.pth
Saved 0 rows to /local/outputs/preds/tid2013/test/preds_hyperiqa.csv
